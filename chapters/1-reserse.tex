\chapter{Rešerše}
    \todo{Obecne o resersi, proc jsem se zameril na nasledujici sekce a kam to míří.}
    \blind[4]

    \section{cloud}
        \todo{Definovat cloud a rozdíl oproti inhouse, proc je to vhodne pro CI/CD}
        \blind[2]

    \section{Dostupnost aplikace}
        Fishman (Sun Microsystems Inc.) definuje dostupnost jako vztah mezi střední čas mezi výpadky (MTBF) a dobou zotavení z výpadku (MTTR) \cite{fishman-availability}. Dostupnost se často vulgarizuje na „počet devítek“ (99,999 \% dostupnost odpovídá pěti devítkám, což je kolem 5 minut nedostupnosti ročně). Cílem je teoretickou dostupnost maximalizovat. B.~Wilson z firmy Backblaze ale trefně dodává, že od 8. devítky jsou jakékoliv odhady čistě akadamické a aplikace s větší pravděpodobností nebude fungovat kvůli ozbrojenému konfliktu \cite{backblaze-availability}.

        Aplikaci v jedné instanci na jednom fyzickém serveru lze bez výpadku provozovat. Jedním takový příkladem je server Stratus s redundantním hardwarem, který měl v jednom případě rekordní uptime 24 let \cite{thibodeau-longest-uptime}. Celý systém běží na proprietárním operačním systému, který od let 2000 nebyl aktualizovaný. Tyto servery jsou ale fyzicky na jednom místě a při přírodní pohromě nebo delším výpadku elektřiny přestanou být dostupné.

        Řešení je aplikaci provozovat distribuovaně, v několika fyzicky vzdálených datacentrech. V případě výpadku části systému se uživatelské požadavky plynule rozdělí mezi zbylé funkční systémy. Pro uživatele je tato operace transparentní a neprojeví se zhoršením dostupnosti. Provozovat a později aktualizovat distribuovanou aplikaci je relativně složité a více to popisuji v kapitole \ref{distributed-apps}.

        Při testování CI/CD systémů se budu zajímat o několik různých dostupností:
        \begin{itemize}
            \item Dostupnost CI/CD systémů při běžném provozu;
            \item Dostupnost CI/CD systémů při správě, typicky při aktualizaci na novější verzi;
            \item Podpora nasazení uživatelské aplikace v daném CI/CD a dopad na dostupnost dané aplikace.
        \end{itemize}

    \section{testování}
        \todo{Definovat testování v cloud}
        \todo{Popsat, jak lze snadno integrovat další služby jako například Messenger}
        \blind[4]

    \section{deployment}
        \todo{citovat nejaky whitepaper co je to deploy}\blind[1]

        V této sekci popisuji varianty nasazení nové verze aplikace s vysokou dostupností. U porovnávaných CI/CD systémů budu později zkoumat, jaké varianty nasazení podporují, případně jak obtížné je jednotlivé varianty implementovat.

        Konceptuálně je nasazení webového sofware přímočaré: vystavěný balíček aplikace se nějakým způsobem nahraje na jeden nebo více serverů a následně se spustí. Detaily se liší podle použitého prostředí. V následujícím textu jsem se zaměřil na monolitické webové aplikace, ale podobné principy fungují i pro nasazení dílčích podpůrných služeb (\textit{microservices}), které mimo HTTP rozhraní mohou nabízet například gRPC, Java RMI, nebo jakékoliv jiné textové nebo binární protokoly.

        Komplexita nasazení drasticky narůstá, pokud vyžadujeme kontinuální dostupnost aplikace. V ideálním případě by aplikace měla úspěšně obsloužit všechny příchozí požadavky. Pokud neuvažujeme continuous deployment a nasazování probíhá výjimečně, může nasazení nové verze spočívat v zastavení původní verze, výpadku, a nasazení nové verze. Toto dodnes některé firmy praktikují. Například všichni tři velcí telefonní operátoři měli v roce 2018 minimálně jednu několikahodinovou odstávku webových služeb (T-Mobile \cite{tmobile-odstavka}, O2 \cite{o2-odstavka}, Vodafone 2. května 2018 a tiskovou zprávu z webu smazal). V následujícím textu se zaměřím pouze na nasazování bez výpadku dostupnosti aplikace.

        \subsection{Hot swapping}
            Jedna varianta kontinuální dostupnosti je hot-swapping, při kterém se aplikace může upravit bez nutnosti ji vypnout. Toto je ale dostupné pouze v prostředí s JVM a dále například v -- na webu málo používaných -- Lisp, Erlang, Smalltalk a Elm. Při použití hot-swapping může aplikace běžet pouze v jedné instanci a nasazení nové verze aplikace tak můžeme udělat atomicky. Tím se vyhneme celé řadě problémů popsaných níže, na druhou stranu ale výrazně zhoršujeme praktickou dostupnost: server na kterém aplikace běží musí být vždy dostupný.

        \subsection{Jednorázový přeliv provozu mezi starou a novou aplikací}
            Alternativní řešení je spustit novou verzi aplikace souběžně s původní verzí a následně přesunout provoz ze staré instance na novou. V případě PHP-FPM při zapnuté OPcache například můžeme přepsat soubory na disku a novou verzi aplikace nasadit smazáním OPcache (což se dělá pomocí rolling update PHP-FPM workerů). Ve světě Node.js lze využít PM2, který abstrahuje spouštění a vypínání procesů aplikace. \todo{ještě jeden, třeba python}. Všechny tyto systémy pracují na principu Load Balanceru: uživatelské požadavky chodí na jeden vedoucí proces, který si ukládá informace o běžících aplikacích na pozadí a požadavky na ně přeposílá. Tento princip je implementovatelný pro libovolnou aplikaci. Pro load balancing můžeme použít celou řadu software: PFSense, HAProxy, Nginx, Varnish, v pokročilejších infrastrukturách Envoy \textit{service mesh}, \ldots Důležité rozhodnutí při výběru LB je balancovaná komunikační vrstva. Layer 4 (transportní vrtva OSI modelu) LB přeposílají TCP požadavky na základě cílové IP adresy a portu. Je definován i Layer 3 LB, který se rozhoduje pouze podle cílové IP, ale v literatuře se zahrnuje do L4. Layer 7 (aplikační vrstva) LB přeposílají celé HTTP požadavky. Z vyššího protokolu vyčtou víc informací, například doménu nebo jiné hlavičky, a při přeposílání požadavku tak mohou vybrat ze celé skupiny různých serverů ty, kde daná aplikace běží. Dále L7LB musí řešit HTTPS (TLS termination) a musí tak dopředu vědět, jaké domény obsluhuje. L7 NB dále může reagovat i na získané HTTP odpovědi. Díky HTTP specifikaci idempotentních metod (GET, HEAD, PUT, DELETE \cite{http-idempotent}) lze některé požadavky zkusit znovu na jiném serveru a to bez toho, aby se původní chyba vrátila uživateli. Load balancer může u každého serveru na pozadí měřit chybovost a rozbité servery z rotate dočasně vyřadit \todo{cite}.

            Jak L4 tak L7 LB často podporují \textit{PROXY protocol} \cite{tarreau-proxyprotocol}, který požadavek obalí vlastní hlavičkou, které je mj. původní IP adresa uživatele. V případě čistého L7 load balancování lze původní IP vyčíst z problematické \cite{hansen-xforwardedfor} hlavičky X-Forwarded-For nebo z Forwarded rozšíření \cite{http-forwarded}. U čistého L4 load balancování o původní IP příjdeme bez PROXY protokolu úplně.

            Je nutné uvažovat konzistenci napříč několika příchozími HTTP requesty: uživatel může načíst HTML z původní verze, ale následující HTTP požadavek na kaskádové styly už může přijít na novou verzi aplikace.

            Tento postup můžeme aplikovat i na aplikace běžící ve víc než jedné replice. Spustíme novou verzi aplikace souběžně se startou verzí a atomicky aktualizujeme na load balanceru cíle na novou skupinu serverů. Stále ale nemusíme řešit většinu problémů, které by přinesl distribuovaný systém.

        \subsection{Distribuovaná aplikace}
            \label{distributed-apps}
            Postup z předchozí sekce vyžaduje atomickou změnu konfigurace load balanceru. To je velmi těžké docílit, pokud máme více než jednu instanci load balanceru \todo{cite}. Další nevýhodou je, že při nasazování nové verze aplikace se po nějakou dobu zdvojnásobí nutné výpočetní zdroje (musí běžet současně $n$ starý a $n$ nových instancí aplikace). Při continuous deployment to může být velmi často, což se prodraží na nezbytné infrastruktuře. Praktický problém tohoto řešení je i samotná jednorázovost přelivu uživatelů: pokud je část aplikace rozbitá, je rozbitá pro 100 \% uživatelů.

            Vhodnější řešení je tzv. rolling update, kdy souběžně nějakou dobu provozujeme novou i starou verzi aplikace v narůstajícím poměru. Průběžně monitorujeme aplikační metriky a v případě, že se nová verze aplikace nechová podle očekávaní, můžeme nasazování pozastavit nebo úplně vrátit. Pracujeme tedy s následujícími předpoklady:
            \begin{itemize}
                \item Aplikace zároveň běží ve staré i nové verzi. Uživatelé mohou být obsluhováni novou a následně starou verzí aplikace.  Můžeme na úrovni LB částečně zajistit, aby uživatelé v po sobě jdoucích požadavcích byly nasměrování na stejnou verzi aplikace (\textit{session affinity, session persistence}), ale není to spolehlivé a 100 \% řešení. Používá se buď sledovací cookie nebo IP adresa a oboje může uživatel upravit. Kromě toho chceme umožnit tzv. \textit{rollback}, při kterém se aplikace downgraduje na předchozí verzi.
                \item Rozdělení požadavků mezi aplikace řeší několik load balancerů. Máme vysokou dostupnost, ale ztratili jsme možnost atomických úprav konfigurace.
            \end{itemize}

            Můžeme libovolně nastavit nejenom poměr aplikací, ale i jejich absolutní počty. Pokud snížíme teoretickou dostupnost, můžeme z $n$ starých verzí aplikací vypnout $n-1$ (tzn. 1 instance staré aplikace stále běží a obsluhuje 100 \% požadavků, nenastal výpadek) a následně zapnout $n-1$ nových instancí. Tím jsme minimalizovali počet nutných výpočetních zdrojů. Pokud ale máme k dispozici volné zdroje, můžeme nechat běžících $n$ původních instancí a nasadit $m$ nových. Například v \textit{Kubernetes} je tato logika abstrahována do nastavení očekávaného počtu instancí, minimálního počtu dostupných instancí a maximálního nárůstu. Při nasazování se pak postupně vypínají staré aplikace, čeká se na zapnutí nových a tak dokola, než jsou všechny instance nové aplikace. Z tohoto procesu pochází název \textit{rolling update}.

            Za těchto podmínek nemůžeme nasazovat libovolnou aplikaci. Obecně musí být aplikace zpětně nebo dopředně kompatibilní. To komplikuje databázové migrace, nebo jinou změnu formátu na sdílených (především persistentních) úložištích. Některé změny je tak nutné nasazovat dvoufázově: první nasadíme úpravu, která zajistí dopřednou kompatibilitu, a poté samotné (původně nekompatibilní) změny.


    \section{definice CI/CD}
        \subsection{Continuous integration}
            Nejstarší zmínka o CI je projekt \textit{Infuse} z roku 1989 \cite{kaiser-infuse}. Jde o návrh systému testování komplexních modulárních projektů, který podle hierarchie spouští postupně jednotkové, integrační a akceptační testy.

            Fowler v článku z roku 2000 \cite{fowler-ci-original} definuje CI jako praktiku, při které vývojáři změny začleňují do sdíleného kódu jednou denně nebo častěji. Logická funkčnost (ale ne nutně úplnost byznysových funkcní) je pak po každé změně otestována automatickými testy. Pokud testy selžou, vývojář jehož změny zapříčinily při testování problémy je informován a očekává se, že chyby opraví.

            Na continuous integration navazuje celá řada praktik: feature branches \todo{seznam}.

            V aktualizovaném článku z roku 2006 Fowler \cite{fowler-ci} zmiňuje užitečnost CI serveru jako volitelnou -- ale praktickou -- podporu této praktiky. Na vhodný výběr CI serveru se soustředí tato práce.

        \subsection{Continuous delivery}
            \todo{define}
            Chad Wathington dále definuje Continuous integration jako podmnožinu CD \cite{fowler-go}.
            \blind[2]

        \subsection{Continuous deployment}
            \todo{definovat}\blind[2]

    \section{kontejnerizase, docker}
        \todo{Popsat co to je a proc je to vhodne na testovani a deploy}
        \blind[2]
